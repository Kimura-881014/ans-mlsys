{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JxHytEZRUA2"
      },
      "source": [
        "### 最適化アルゴリズム\n",
        "ニューラルネットワークなど全ての機械学習の到達目標はモデルの重みを調整し、損失をゼロにすることであり、効率よく損失をゼロにするのが最適化アルゴリズムである。一般的によく用いられる最適化アルゴリズムとして、AdamやSGDなどがある。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyKbt4hcRhUQ"
      },
      "source": [
        "### バッチとエポック\n",
        "バッチは入力と正解のペア(データの要素、サンプル)の集合であり、バッチごとに学習が行われる。  \n",
        "エポックはすべてのデータを1通り学習することであり、1エポックですべての訓練データを使い切る。\n",
        "通常1エポックは複数のバッチで構成される。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaTtb9JoRkkE"
      },
      "source": [
        "### 畳み込み層\n",
        "画像を扱う際に用いることの多い層である。  \n",
        "カーネルと呼ばれる畳み込み演算時に用いる小さな行列の重みを更新する。カーネルと入力データとの積を特徴マップといい、うまく学習させると入力データの特徴を抽出できるようになる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjBv7V_wdlBN"
      },
      "source": [
        "### 2010年頃まで、層数の多いネットワークはタブーであった  \n",
        "\n",
        "#### なぜ層数の多いネットワークは利用されなかったのか  \n",
        "- 活性化関数にシグモイド関数を用いることが多かったなどの理由から層数の多いネットワークでは勾配消失問題が発生し、当時の限られた計算リソースでは計算コストが大きい割に精度が上がらずメリットがなかっためである。\n",
        "\n",
        "#### どのような技術が層数の拡大を可能としたのか  \n",
        "- max(0,x) を活性化関数に用いて、長いネットワークにおける勾配消失問題に対応できるようになった(ReLU)  \n",
        "- ランダムに学習に用いる際のネットワーク結合の計算を意図的にスキップすることで、正則化効果が得られる(Dropout)  \n",
        "- あるブロック(複数のレイヤの纏まり)の出力に、そのブロックへの入力を足すという構造を持たせる(Resideual Learning)  \n",
        "- 層を大きくしても高性能なGPUを複数用いることで計算ができるようになった\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
